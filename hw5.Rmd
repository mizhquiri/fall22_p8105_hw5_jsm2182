---
title: "HW 5"
output: github_document
---

```{r, include = FALSE, messsage=FALSE, warning=FALSE}
library(tidyverse)


knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))


options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d


set.seed(1) #sets a seed for reproducibility
```

## Problem 2

Load data 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
homicide_rawdf = read_csv("./data/homicide-data.csv")
```
This dataset overviews demographic and geographic data for homicide victims across various cities in the United States. 

```{r}
homicide_cleandf = 
  homicide_rawdf %>% 
  mutate(
    city_state = str_c(city,", ", state)
  )
```


```{r}
all_homicides = 
  homicide_cleandf %>% 
  group_by(city_state) %>% 
  summarize(n_obs = n())
```
```{r}
unsolved_homicides = 
  homicide_cleandf %>% 
  group_by(city_state) %>% 
  filter(
    disposition != "Closed by arrest"
  ) %>% 
  summarize(
    n_obs_unsolved = n()
  )
```

```{r}
homicide_city_state_data = 
  left_join(all_homicides, unsolved_homicides, by = "city_state")

head(homicide_city_state_data)
```



_Baltimore 1 - sample Proportion of Unsolved Homicides to All Homicides_
```{r}

  baltimore_homicides = 
  homicide_city_state_data %>% 
  filter(
    city_state == "Baltimore, MD"
  ) 


  prop.test(baltimore_homicides %>% pull(n_obs_unsolved), baltimore_homicides %>% pull(n_obs)) %>% 
  broom::tidy() %>% 
  select(
    estimate, conf.low, conf.high
  )

```


```{r}
  prop.test(baltimore_homicides %>% pull(n_obs_unsolved), baltimore_homicides %>% pull(n_obs)) %>% 
  broom::tidy() %>% 
  select(
    estimate, conf.low, conf.high
  )

```


We are 95% confident that the true proportion unsolved homicide rates in Baltimore is between 0.627 and 0.663.

1. Create a function that computes estimate, conf.low, conf.high

%>%  version
```{r}

prop_test_output = function(df) {
  

  prop.test(x = df %>% pull(n_obs_unsolved), n = df %>% pull(n_obs))

}


```

2. map the data


```{r, eval = FALSE}

homicide_city_state_data %>% 
  rowwise(city_state) %>% 
  mutate(
    proportion_test = purrr::map2(x = homicide_city_state_data %>% pull(n_obs_unsolved), y = homicide_city_state_data %>% pull(n_obs), ~prop.test(x = .x, n = .y))
    ) 


```


```{r, eval = FALSE}
homicide_city_state_data %>% 
  mutate(
    prop = map2(.x = n_obs_unsolved, .y = n_obs, ~prop_test_output(x = .x, n = .y))
  )
```


```{r}
sample = rnorm(30, mean = 0)

test_results = t.test(sample) %>% 
  broom::tidy()
```
```{r}
sim_t_test = function(true_mean) {
  sample = rnorm(30, mean = 0)
  
  test_results = t.test(sample) %>% 
  broom::tidy()
}

expand_grid(
  true_mean = 0:6, 
  iter = 1:5
)
```

Above, diff sample/diff sample mean but consistent true mean which is the input

Once you have the plot, it's all after you run the simulation 
group_by + summarize for the true mean & summarize how often the null was rejected

group_by averages 

If all you see are results where p< 0.5 --> publication bias; + low power 
 You end up seeing results which are different than true 
 
 
## Problem 3

```{r}
sample = rnorm(n = 30, mean = 0, sd = 5)

```


```{r, eval = FALSE}
sim_results_df = 
  expand_grid(
    sample_size = c(30, 60, 120, 240),
    true_sd = c(6, 3),
    iter = 1:1000
  ) %>% 
  mutate(
    estimate_df = 
      map2(.x = sample_size, .y = true_sd, ~sim_mean_sd(n = .x, sigma = .y))
  ) %>% 
  unnest(estimate_df)

```

 
 
 
 
